{"cells":[{"cell_type":"code","source":["%tensorflow_version 2.x\n","%pip install -q transformers\n","\n","import tensorflow as tf\n","from urllib.request import urlretrieve\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from transformers import BertTokenizer\n","from transformers import TFBertModel\n","from tensorflow.keras.layers import Dropout, Dense\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.metrics import SparseCategoricalAccuracy\n","\n","model_name = \"bert-base-cased\"\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","\n","SNIPS_DATA_BASE_URL = (\n","    \"https://github.com/ogrisel/slot_filling_and_intent_detection_of_SLU/blob/\"\n","    \"master/data/snips/\"\n",")\n","for filename in [\"train\", \"valid\", \"test\", \"vocab.intent\", \"vocab.slot\"]:\n","    path = Path(filename)\n","    if not path.exists():\n","        print(f\"Downloading {filename}...\")\n","        urlretrieve(SNIPS_DATA_BASE_URL + filename + \"?raw=true\", path)\n","\n","\n","def parse_line(line):\n","    data, intent_label = line.split(\" <=> \")\n","    items = data.split()\n","    words = [item.rsplit(\":\", 1)[0]for item in items]\n","    word_labels = [item.rsplit(\":\", 1)[1]for item in items]\n","    return {\n","        \"intent_label\": intent_label, \n","        \"words\": \" \".join(words),\n","        \"word_labels\": \" \".join(word_labels),\n","        \"length\": len(words),\n","    }\n","\n","def encode_dataset(text_sequences):\n","    # encode : vector representation of words in a vocabulary of size n (n = total # of words in vocabulary)\n","    # Create token_ids array (initialized to all zeros), where \n","    # rows are a sequence and columns are encoding ids\n","    # of each token in given sequence.\n","    token_ids = np.zeros(shape=(len(text_sequences), max_token_len),\n","                         dtype=np.int32)\n","    \n","    for i, text_sequence in enumerate(text_sequences):\n","        encoded = tokenizer.encode(text_sequence)\n","        token_ids[i, 0:len(encoded)] = encoded\n","\n","    attention_masks = (token_ids != 0).astype(np.int32)\n","    return {\"input_ids\": token_ids, \"attention_masks\": attention_masks}\n","\n","\n","train_lines = Path(\"train\").read_text().strip().splitlines()\n","valid_lines = Path(\"valid\").read_text().strip().splitlines()\n","test_lines = Path(\"test\").read_text().strip().splitlines()\n","\n","df_train = pd.DataFrame([parse_line(line) for line in train_lines])\n","df_valid = pd.DataFrame([parse_line(line) for line in valid_lines])\n","df_test = pd.DataFrame([parse_line(line) for line in test_lines])\n","\n","max_token_len = 43\n","\n","encoded_train = encode_dataset(df_train[\"words\"])\n","encoded_valid = encode_dataset(df_valid[\"words\"])\n","encoded_test = encode_dataset(df_test[\"words\"])\n","\n","intent_names = Path(\"vocab.intent\").read_text().split()\n","intent_map = dict((label, idx) for idx, label in enumerate(intent_names))\n","intent_train = df_train[\"intent_label\"].map(intent_map).values\n","intent_valid = df_valid[\"intent_label\"].map(intent_map).values\n","intent_test = df_test[\"intent_label\"].map(intent_map).values\n","\n","base_bert_model = TFBertModel.from_pretrained(\"bert-base-cased\")\n","\n","# Build a map from slot name to a unique id.\n","slot_names = [\"[PAD]\"] + Path(\"vocab.slot\").read_text().strip().splitlines()\n","slot_map = {}\n","for label in slot_names:\n","    slot_map[label] = len(slot_map)\n","\n","\n","# Uses the slot_map of slot name to unique id, defined above, as well\n","# as the BERT tokenizer, to create a np array with each row corresponding\n","# to a given sequence, and the columns as the id of the given token slot labels.\n","def encode_token_labels(text_sequences, true_word_labels):\n","    encoded = np.zeros(shape=(len(text_sequences), max_token_len), dtype=np.int32)\n","    for i, (text_sequence, word_labels) in enumerate(zip(text_sequences, true_word_labels)):\n","        encoded_labels = []\n","        for word, word_label in zip(text_sequence.split(), word_labels.split()):\n","            tokens = tokenizer.tokenize(word)\n","            encoded_labels.append(slot_map[word_label])\n","            expand_label = word_label.replace(\"B-\", \"I-\")\n","            if not expand_label in slot_map:\n","                expand_label = word_label\n","            encoded_labels.extend([slot_map[expand_label]] * (len(tokens) - 1))\n","        encoded[i, 1:len(encoded_labels) + 1] = encoded_labels\n","    return encoded\n","\n","\n","# Encode the token labels and store in variables slot_train, slot_valid, slot_test.\n","### YOUR CODE HERE ###\n","\n","slot_train = encode_token_labels(df_train['words'], df_train['word_labels'])\n","slot_valid = encode_token_labels(df_valid['words'], df_valid['word_labels'])\n","slot_test = encode_token_labels(df_test['words'], df_test['word_labels'])\n","\n","# Define the class for the model that will create predictions\n","# for the overall intent of a sequence, as well as the NER token labels.\n","class JointIntentAndSlotFillingModel(tf.keras.Model):\n","\n","    def __init__(self, intent_num_labels=None, slot_num_labels=None,\n","                dropout_prob=0.1):\n","        super().__init__(name=\"joint_intent_slot\")\n","\n","        self.bert = base_bert_model\n","        \n","        # TODO: define the dropout, intent & slot classifier layers\n","        self.dropout = Dropout(dropout_prob)\n","        self.intent_classifier = Dense(intent_num_labels, name=\"intent_classifier\") \n","        self.slot_classifier = Dense(slot_num_labels, name=\"slot_classifier\") \n","\n","    def call(self, inputs, **kwargs):\n","        #original: tokens_output, pooled_output = self.bert(inputs, attention_mask = kwargs.get(\"attention_mask\")) ### YOUR CODE HERE ###\n","\n","        #new:\n","        tokens_output = self.bert(inputs[\"input_ids\"], attention_mask=inputs[\"attention_masks\"])[0]\n","        pooled_output = self.bert(inputs[\"input_ids\"], attention_mask=inputs[\"attention_masks\"])[1]\n","\n","        # TODO: use the new layers to predict slot class (logits) for each\n","        # token position in input sequence (size: (batch_size, seq_len, slot_num_labels)).\n","        tokens_output = self.dropout(tokens_output, \\\n","                                     training=kwargs.get(\"training\", False))  \n","        slot_logits = self.slot_classifier(tokens_output)  \n","\n","        #new:\n","        return slot_logits, pooled_output\n","\n","# TODO: create an instantiation of this model\n","joint_model = JointIntentAndSlotFillingModel(len(intent_map), len(slot_map))\n","\n","# Define one classification loss for each output (intent & NER):\n","losses = [SparseCategoricalCrossentropy(from_logits=True),\n","          SparseCategoricalCrossentropy(from_logits=True)]\n","          \n","joint_model.compile(optimizer=Adam(learning_rate=3e-5, epsilon=1e-08),\n","                    loss=losses,\n","                    metrics=[SparseCategoricalAccuracy('accuracy')])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aPqktAYe2g9q","executionInfo":{"status":"ok","timestamp":1685071230482,"user_tz":-540,"elapsed":29694,"user":{"displayName":"­편예빈 / 학생 / 자유전공학부","userId":"01368699004970672654"}},"outputId":"e7d6be61-3b5f-4cfa-f3a9-842ffb8af9b8"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]}]},{"cell_type":"code","source":["# Train the model.\n","history = joint_model.fit(encoded_train, \\\n","    (slot_train, intent_train), \\\n","    validation_data=(encoded_valid, (slot_valid, intent_valid)), \\\n","    epochs=1, batch_size=32)\n","\n","# Use the model we trained to get the intent & slot logits\n","# and print the actual string of the class corresponding to\n","# highest logit score for each token, and the sentence overall.\n","def show_predictions(text, intent_names, slot_names):\n","    inputs = tf.constant(tokenizer.encode(text))[None, :]  # batch_size = 1\n","    outputs = joint_model(inputs) ### YOUR CODE HERE ###\n","    slot_logits, intent_logits = outputs  ### YOUR CODE HERE ###\n","    slot_ids = slot_logits.numpy().argmax(axis=-1)[0, 1:-1]\n","    intent_id = intent_logits.numpy().argmax(axis=-1)[0]\n","    print(\"## Intent:\", intent_names[intent_id])  ### YOUR CODE HERE ###\n","    print(\"## Slots:\")\n","    for token, slot_id in zip(tokenizer.tokenize(text), slot_ids):\n","        print(f\"{token:>10} : {slot_names[slot_id]}\")\n","\n","\n","def decode_predictions(text, intent_names, slot_names,\n","                       intent_id, slot_ids):\n","    info = {\"intent\": intent_names[intent_id]}\n","    collected_slots = {}\n","    active_slot_words = []\n","    active_slot_name = None\n","    for word in text.split():\n","        tokens = tokenizer.tokenize(word)\n","        current_word_slot_ids = slot_ids[:len(tokens)]\n","        slot_ids = slot_ids[len(tokens):]\n","        current_word_slot_name = slot_names[current_word_slot_ids[0]]\n","        if current_word_slot_name == \"O\":\n","            if active_slot_name:\n","                collected_slots[active_slot_name] = \" \".join(active_slot_words)\n","                active_slot_words = []\n","                active_slot_name = None\n","        else:\n","            # Naive BIO: handling: treat B- and I- the same...\n","            new_slot_name = current_word_slot_name[2:]\n","            if active_slot_name is None:\n","                active_slot_words.append(word)\n","                active_slot_name = new_slot_name\n","            elif new_slot_name == active_slot_name:\n","                active_slot_words.append(word)\n","            else:\n","                collected_slots[active_slot_name] = \" \".join(active_slot_words)\n","                active_slot_words = [word]\n","                active_slot_name = new_slot_name\n","    if active_slot_name:\n","        collected_slots[active_slot_name] = \" \".join(active_slot_words)\n","    info[\"slots\"] = collected_slots\n","    return info\n","\n","def nlu(text, intent_names, slot_names):\n","    inputs = tf.constant(tokenizer.encode(text))[None, :]  # batch_size = 1\n","    outputs = joint_model(inputs)\n","    slot_logits, intent_logits = outputs\n","    slot_ids = slot_logits.numpy().argmax(axis=-1)[0, 1:-1]\n","    intent_id = intent_logits.numpy().argmax(axis=-1)[0]\n","\n","    return decode_predictions(text, intent_names, slot_names, intent_id, slot_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2iKbXPC924Qi","executionInfo":{"status":"ok","timestamp":1685079978986,"user_tz":-540,"elapsed":97568,"user":{"displayName":"­편예빈 / 학생 / 자유전공학부","userId":"01368699004970672654"}},"outputId":"85ee5ef4-f01e-4835-b088-c901eb0469eb"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["409/409 [==============================] - 8444s 21s/step - loss: 5.1907 - output_1_loss: 0.2542 - output_2_loss: 4.9366 - output_1_accuracy: 0.9437 - output_2_accuracy: 0.4437 - val_loss: 4.7348 - val_output_1_loss: 0.0263 - val_output_2_loss: 4.7085 - val_output_1_accuracy: 0.9925 - val_output_2_accuracy: 0.8129\n"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPmbQoSaQQ1LsvNtshEKEOG"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}